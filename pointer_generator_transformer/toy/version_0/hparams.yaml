arch: pointer_generator_transformer
beam_width: 1
beta1: 0.9
beta2: 0.999
bidirectional: true
decoder_layers: 1
dropout: 0.2
embedding_size: 128
encoder_layers: 1
end_idx: 3
expert: null
features_attention_heads: 1
features_encoder_cls: null
features_vocab_size: 6
hidden_size: 512
label_smoothing: 0.0
learning_rate: 0.001
max_source_length: 128
max_target_length: 128
optimizer: adam
output_size: 6
pad_idx: 1
scheduler: lineardecay
scheduler_kwargs:
  check_val_every_n_epoch: 1
  end_factor: 1.0
  min_learning_rate: 0.0
  reduceonplateau_factor: 0.1
  reduceonplateau_mode: loss
  reduceonplateau_patience: 10
  start_factor: 0.3333333333333333
  total_decay_steps: 5
  warmup_steps: 0
source_attention_heads: 4
source_encoder_cls: !!python/name:yoyodyne.models.modules.transformer.TransformerEncoder ''
source_vocab_size: 5
start_idx: 2
target_vocab_size: 6
teacher_forcing: true
